---
title: "notes - 1/28"
format: html
editor: visual
---

# Notes 1/28

**Five Number Summary** is *min, max, 1st, 2nd, and 3rd quartiles*

### Challenge

Number of variables: 16

Number of observations: 248

Number of countries: 248

Densest country: Macau

# Notes 1/30

``` r
file2 <- file.choose()
d2 <- read_csv(file2, col_names = TRUE)
d2$population
d2$area
d2$density <- d2$population / d2$area
d2 <- d2 %>%
  arrange(desc(density))
##requires read_csv to work properly
```

Important to understand the difference between a **data frame** (data arranged into a two dimensional table)and a **tibble** (list of vectors)

-   Tibbles don't work with as many functions as data frames, so those may cause more errors. However, certain functions work only on tibbles

    -   Most sub-setting works on both though

-   Tibbles are more manageable for working with in general (Tony's preference)

Useful functions for getting a grasp of the data you just freeking imported

``` r
head()
tail()
colnames()
rownames() ##only works with df
str()
glimpse()
dim()

attach() #allows R to know which variable you are referring to without specifying the data frame
with() #accomplishes a simular goal
```

Summaries for variables

``` r
summary() #gives the 5-number summary for each variable plus its class & mode if applicable
skim() #more cleaned up version of summary()
```

Boxplots & Sexy Boxplots (stripplots)

``` r
boxplot(log(d3$Body_mass_female_mean))
stripchart(log(d3$Body_mass_female_mean),
           method = "jitter",
           col = "blue",
           vertical = TRUE,
           add = TRUE)
boxplot(log(d3$Body_mass_female_mean) ~ d3$Family)
stripchart(log(d3$Body_mass_female_mean) ~ d3$Family,
           method = "jitter",
           col = "blue",
           vertical = TRUE,
           add = TRUE)
```

ggplot2 and its mysteries

``` r
p <- ggplot(data = d3,
       aes(x = "", y = log(d3$Body_mass_female_mean))) + 
  geom_boxplot(na.rm = TRUE) + 
  geom_jitter( color = "blue", width = 0.1)
```

# Notes 2/4

Scatterplot

``` r
library(tidyverse)
d <- read_csv("/Users/juneburke/Downloads/KamilarAndCooperData.csv", col_names = TRUE)
attach(d)
(p <- ggplot(data = d, aes(x = log(Body_mass_female_mean),
                          y = log(Brain_Size_Female_Mean))) + 
  geom_point(na.rm = TRUE)) ##surrounding it will make the plot automatically appear
str(p)
```

Adding stuff

``` r
(p <- ggplot(data = d, aes(x = log(Body_mass_female_mean),
                          y = log(Brain_Size_Female_Mean))) + 
  geom_point(na.rm = TRUE) + 
    geom_smooth(method = "lm", na.rm=TRUE, color = "pink") +
    geom_vline(xintercept = 7)+
    geom_hline(yintercept = 3)+
    geom_point(data = d, aes( x=log(Body_mass_female_mean),y=log(Body_mass_male_mean)))+
    geom_smooth(data = d, aes(
      x=log(Body_mass_female_mean), 
      y=log(Body_mass_male_mean)
      ), method = "lm", na.rm = TRUE, color = "cyan"))
install.packages("cowplot")
library(cowplot) 
plot_grid()#lets you load multiple plots together
```

**Data wrangling**

``` r
filter() #extracts rows that meet logical criteria (exports as tabular data)
select() #extract columns as a table (exports as tabular data)
arrange() #order rows by values of a column/columns from low to high
  ##adding desc() switches the order
group_by() #creates a table grouped by a particular column
  ##piping helps to simplify this process
summarize() #gets summaru statistics for a certain column as a vector
mutate() #creates new variables within a dataframe
```

*Examples*

``` r
s <- select(d, Family, Genus, Body_mass_male_mean)
s <- arrange(d, Family, Genus, desc(Body_mass_male_mean))
s <- summarize(
  group_by(d, Family),
  avgF = mean(Body_mass_female_mean, na.rm = TRUE)
)
```

**Flow Control & Looping**

``` r
if () else ()
ifelse()
if_else()
```

# Notes 2/11

Flow Control and Conditional Statements

``` r
#looping
for (i in 1:10) {
  print(i)
}

i <- 1
while (i <= 10) {
  print(1)
  i <- i+1
}
```

### Joins

An inner join connects along a particular index, basically removing anything that doesn't match across both tables

An outer left join creates a NA cell if there is no info in the right table that matches the left table and ignores vice versa from the left table.

An outer right join does the opposite and a full outer does both

``` r
inner <-inner_join(c, p, by = c("fullName" = "First Author"))
left <- left_join(c,p, by = c("fullName" = "First Author"))
right <- right_join(p,c, by = c("First Author" = "fullName"))
find_pubs <- tibble(fullName = c("Abbott, David H"))
inner2 <- inner_join(find_pubs,p, 
                     by = c("fullName" = "First Author"))
##fuzzy joins - not exact uses a lot of regular expressions
find_pubs2 <- tibble(partialName = c("^Abbott"))
inner_fuzzy <- regex_inner_join(p,find_pubs2, by = c("First Author" = "partialName"))
find_pubs3 <- tibble(partialName = c("^Mea", "ony$"))
inner_fuzzy2 <- regex_inner_join(p, find_pubs3, by = c("First Author" =
                                                       "partialName"))
```

"\^start" or "end\$" = regular expression. Allows for fuzzy connections (no longer case sensitive and has some leeway

### Functions

``` r
##template
my_function <- function(<argument list>){
<<function code>>
return(<value>)
}
#example
```

# Notes 2/13

Data Wrangling Challenge

``` r
library(tidyverse)
install.packages("oce")
library(oce)
library(dplyr)
gps <- read_csv("https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/sample_gps_data.csv", col_names = TRUE)
beh <- read_csv("https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/sample_behavioral_data.csv", col_names = TRUE)
beh <- beh |> mutate(Year = substr(beh$Date.Time, 1, 4))
beh <- filter(beh, Year == (2012:2014))
inner <-inner_join(beh, gps, by = c("Date.Time" = "Date.Time", "Observer" = "Observer"))
inner2 <- inner |>
  rowwise() |>
  mutate(
    easting = lonlat2utm(`Mean.Longitude`,`Mean.Latitude`)$easting,
    northing = lonlat2utm (`Mean.Longitude`, `Mean.Latitude`)$northing + 
      10000000
  )
poto <- filter(inner2, Focal.Animal == ("Poto"))
library(ggplot2)
potoplot <- ggplot(data = poto, aes(x = easting,
                           y = northing)) + 
    geom_point(na.rm = TRUE) +
potoplot
```

## big statistics ideas

population vs. sample

*Population* is a group we want to study (all gorillas) whereas the *sample* is a subset of the population that we hope can represent the entire population (1000 randomly selected gorillas)

parameter vs. statistics

A *statistic* is some function of the data alone based on a finite amount of the data (usually the sample) whereas the *parameter* is supposed to represent the bounds of a given population

measures of location

**mean** (avg.)

**median** (middle number)

**mode** (common number)**,**

**harmonic mean** (the reciprocal of the average of the reciprocals of a set of values)

measures of spread

**mean deviation** (average of absolute values of deviations from median),

**mean squared deviation** (average of squared deviations from the mean) also called **variation,**

-   *population variance* - a parameter (sigma squared)

    ``` r
    sum((x - mean(x))^2)/(length(x))
    ```

-   *sample variance* - a statistic (s squared)

    ``` r
    var()
    ```

**standard deviation** (square root of variance).

measures of shape

**skewness** (characterizes the asymmetry of the distribution)

**kurtosis** (characterizes the peakedness/flatness of a distribution \[compared to normal\])

# Notes 2/18

## more big statistics ideas

classical/frequentist statistical inference is based on well-define mathematical distributions. essentially, we assume they they are estimates of the parameters (mean is an estimate of mu - sd is an estimate of sigma - etc) of distribution

*Exercise*

``` r
library(mosaic)
mu <- 10
sigma <- 2
plotDist("norm",mean=mu, sd=sigma, xlab = "x", ylab = "Frequency")
##visualizing a normal distribution ^
##drawing out of the normal distribution v 
s1 <- rnorm(n = 100, mean = 10, sd = 2)
mean(s1)
sd(s1)
```

**sampling distribution** is set of possible statistics that could have been generated if the data collection process were repeated many times, along with the probabilities of these possible values

``` r
reps <- 500
samp_dist_mean <-
  do(reps) * mean(rnorm(n=10, mean=10, sd =2))
str(samp_dist_mean)
histogram(samp_dist_mean$mean)
samp_dist_median <-
  do(reps) * median(rnorm(n=10, mean=10, sd =2))
histogram(samp_dist_median$median)
mean(samp_dist_mean$mean)
```

How far off is a statistic that we calculate based on a sampling distribution likely to be from the true population value of the parameter of interest?

**standard error** is one measure of this reliability (= square root of the variance of the sampling distribution OR standard deviation of a sampling distribution

``` r
se_mean <- sd(samp)
```

**confidence interval** is another way of describing a statistics sampling distribution or uncertainty. It plays the most important role in inferential statistics. (=n interval around our estimate of mean of the sampling distribution for a particular statistic) produces a range of values into which subsequent estimates of a statistic would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated

## Challenge

``` r
sample <- rnorm(n = 100, mean = 2, sd = 4)
mean(sample) #what is the mean? 1.706305
sd(sample) #what is the standard deviation? 4.374248
(se1 <- sd(sample/sqrt(length(sample)))) #what is the standard error based on the sample? 0.4374248
reps <- 1000
sample_dist_mean <-
  do(reps) * mean(rnorm(n = 100, mean = 2, sd = 4))
(se2 <- sd(sample_dist_mean$mean)) #se for the sample distribution? 0.3893194
plotDist("t", df=10, xlab = "x", ylab = "Frequency", col = "cyan")
reps <- 1000
sample_dist_t <-
  do(reps) * mean(rt(n = 100, df = 99, ncp = 2))
(mean(sample_dist_t$mean))
plotDist("beta", shape1=0.3, shape2=4, xlab = "x", ylab = "Frequency", col = "cyan")
```

# Notes 2/20

## Key functions for distributions

r\_(n=) - draws RANDOM SAMPLES of size n from a given distribution

p\_(q=) - returns the quantile associated with a given value of X (the value of the cumulative density function

q\_(p=) - returns the value of X at a given quantile through the distribution (value of the inverse cumulative density function

d\_(x=) - returns the value of the probability density function at the values of x

**central limit theorem** as the number of samples get bigger, they begin to approach a normal distribution regardless of the original distribution

# Notes 2/27

``` R
```
